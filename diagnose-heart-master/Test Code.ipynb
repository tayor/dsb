{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import SETTINGS as sts;\n",
    "import fitting_models\n",
    "import analysis\n",
    "import train_pred\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train_pred' from 'train_pred.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train cases is 300\n"
     ]
    }
   ],
   "source": [
    "#####Load all data, same in the test code, but we can filter out train in train and test for test\n",
    "info = pd.read_csv(sts.output_dir + '/info.csv')\n",
    "ch4_data = { int(r[0]):(r[1], r[2]) for _,r in\n",
    "      pd.read_csv('tencia_scripts/output/ch4_volumes_map.csv', header=False).iterrows()};\n",
    "#Tencia's cnn result\n",
    "\n",
    "#Qi's cnn result\n",
    "qifiles = [ 'v1_p2090_size256_ss100_nocL_tag3_2norm',\\\n",
    "    'v2_p2090_size196_ss100_nocL_tag6_2norm',\\\n",
    "    'v1_p2090_size256_ss150_nocL_tag10_2norm',\\\n",
    "    'v2_p2090_size196_ss150_nocL_tag11_2norm',\\\n",
    "    'v1_p2090_size256_ss200_nocL_tag5_2norm',\\\n",
    "    'v2_p2090_size196_ss200_nocL_tag7_2norm',\\\n",
    "    'v1_p2090_size256_ss75_nocL_tag12_2norm',\\\n",
    "    'v2_p2090_size196_ss75_nocL_tag13_2norm',\\\n",
    "    ];\n",
    "qi_areas = [analysis.get_cnn_results(sts.output_dir +\"/areas_map_{}.csv\".format(v)) for v in qifiles];\n",
    "qi_cnts = [analysis.get_cnn_results(sts.output_dir +\"/contour_portion_{}.csv\".format(v)) for v in qifiles];\n",
    "\n",
    "train_true = pd.read_csv(sts.data_kaggle + '/train.csv');#append validate result to here once released\n",
    "validate = train_true[train_true.Id>300];\n",
    "train_true = train_true[train_true.Id<=300];\n",
    "\n",
    "Ntrain = train_true.shape[0];#500, becomes 700 when validate data released\n",
    "print(\"number of train cases is {}\".format(Ntrain));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ train sex age model :\n",
      "use fitted values, no fitting\n",
      "score is 0.0393119122528\n",
      " ---- train ch4 model :\n",
      "278 cases are used to fit the model\n",
      "cases are missing in train: 1,12,19,36,60,66,74,85,100,101,116,135,144,166,167,176,191,200,212,216,225,260\n",
      "fitting parameters [ 0.86105281  6.51470415  0.33565059  0.85714133  0.07211585  8.58795949]\n",
      "fitting score 0.0176765079141\n",
      "score is 0.0176765079141\n",
      " --- train oneslice model :\n",
      "not implemented yet, use default to fit\n",
      "score is 0.0155012381802\n",
      " --- building default model :\n",
      "score is 0.0138794226048\n"
     ]
    }
   ],
   "source": [
    "#### train models,\n",
    "########### default models\n",
    "sa_predict = train_pred.train_sex_age_model(info, train_true);\n",
    "ch4_predict = train_pred.train_ch4_model(ch4_data, train_true);\n",
    "pick = [0,1]; \n",
    "qi_best,qi_best_cont = analysis.take_best_contour([qi_areas[i] for i in pick],[qi_cnts[i] for i in pick],method=1);\n",
    "oneslice_pred = train_pred.train_oneslice_model(qi_best,train_true);\n",
    "# fit the fall back model, a combination of oneslice_model and 4-ch model, \n",
    "# if it still fails use the sex-age model\n",
    "# 0.6 * oneslice_predict + 0.4 * ch4_predict; (use fixed 0.6, 0.4)\n",
    "default_pred = train_pred.build_default_model(oneslice_pred, ch4_predict, sa_predict);\n",
    "analysis.evaluate_pred(default_pred, train_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- train sax model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.00479942  0.99180158  0.05414734  4.00012514]\n",
      "fitting score 0.0104732086834\n",
      "score is 0.0104732086834\n"
     ]
    }
   ],
   "source": [
    "tencia_files = ['pMS','p1090'];\n",
    "tencia_areas = [analysis.get_cnn_results('tencia_scripts/output/areas_map_{}.csv'.format(x)) for x in tencia_files];\n",
    "tencia_best = analysis.take_best(tencia_areas, method=2);\n",
    "tencia_predict = train_pred.train_sax_model(tencia_best, train_true, version = 1);\n",
    "#0.01048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- train sax model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "volume 3.7128 is too small, set to 4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.04224151  0.87434184  0.08081928  2.16647547  0.05295136  2.71257429]\n",
      "fitting score 0.00922685434573\n",
      "score is 0.00922685434573\n",
      " ---- train sax countour model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "volume 3.7128 is too small, set to 4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 0.45742235  0.56393121  0.06372138  3.99949918  0.04501477  3.99527151]\n",
      "fitting score 0.00974057241091\n",
      "score is 0.00974057241091\n",
      " ---- train sax countour filter model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "volume 3.7128 is too small, set to 4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.15870022  0.90017129  0.05437383  3.99950635  0.047496    3.99385422]\n",
      "fitting score 0.00923522675788\n",
      "score is 0.00923522675788\n"
     ]
    }
   ],
   "source": [
    "pick = [0,1];\n",
    "reload(train_pred)\n",
    "qi_best,qi_best_cont = analysis.take_best_contour([qi_areas[i] for i in pick],[qi_cnts[i] for i in pick],method=1);\n",
    "qi_sax_pred = train_pred.train_sax_model(qi_best,train_true, version = 2, cleaner=[0,1,2]);\n",
    "qi_sax_cnt_pred = train_pred.train_sax_cnt_model(qi_best, qi_best_cont, train_true, version = 2,cleaner=[0,1,2]);\n",
    "qi_sax_filter_pred = train_pred.train_sax_cnt_filter_model(qi_best,qi_best_cont,train_true,cleaner=[0,1,2]);\n",
    "#0,1: 0.009323, 0.00977, 0.009313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length <=4\n",
      "volume 586.382 is too big, set to 580\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 0.27857085 -0.26132326  1.33486723  1.68689225 -2.37452227  0.76255337\n",
      "  4.45235356]\n",
      "fitting score 0.00975271640129\n",
      "score is 0.00975271640129\n"
     ]
    }
   ],
   "source": [
    "reload(analysis)\n",
    "pick = [2,3];\n",
    "reload(train_pred)\n",
    "reload(fitting_models)\n",
    "qi_best,qi_best_cont = analysis.take_best_contour([qi_areas[i] for i in pick],[qi_cnts[i] for i in pick],method=1);\n",
    "\n",
    "result = analysis.get_preliminary_volume_features(qi_best,qi_best_cont,cleaner=[0,1,2]);\n",
    "# result = analysis.get_preliminary_volume_cnt_filter(qi_best,qi_best_cont,cleaner=[0,1,2]);\n",
    "\n",
    "sax_model = fitting_models.SaxFeatureModel();\n",
    "sax_model.fit(result,train_true);\n",
    "sax_predict = sax_model.predict(result);\n",
    "analysis.evaluate_pred(sax_predict, train_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is 0.0106138414263\n",
      "score is 0.0105953784163\n"
     ]
    }
   ],
   "source": [
    "analysis.evaluate_pred(sax_predict, validate);\n",
    "analysis.evaluate_pred(qi_sax_filter_pred2, validate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X =[];\n",
    "for c in result:\n",
    "    X.append(result.get(c));\n",
    "X = np.asarray(X).reshape((-1,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X[:,3]>1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- train sax model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.46320678  1.15553243  0.08932317  1.9810633   0.05198473  3.08080125]\n",
      "fitting score 0.00949225401491\n",
      "score is 0.00949225401491\n",
      " ---- train sax countour model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 0.6649228   0.80098251  0.07321033  3.99754751  0.05216178  3.99733347]\n",
      "fitting score 0.0105702374438\n",
      "score is 0.0105702374438\n",
      " ---- train sax countour filter model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.42213647  1.23801426  0.06721213  4.00052412  0.04858318  3.99610735]\n",
      "fitting score 0.00960771806751\n",
      "score is 0.00960771806751\n"
     ]
    }
   ],
   "source": [
    "pick = [2,3];\n",
    "qi_best,qi_best_cont = analysis.take_best_contour([qi_areas[i] for i in pick],[qi_cnts[i] for i in pick],method=3);\n",
    "qi_sax_pred2 = train_pred.train_sax_model(qi_best,train_true, version = 2,cleaner=[0,1,2]);\n",
    "qi_sax_cnt_pred2 = train_pred.train_sax_cnt_model(qi_best, qi_best_cont, train_true, version = 2,cleaner=[0,1,2]);\n",
    "qi_sax_filter_pred2 = train_pred.train_sax_cnt_filter_model(qi_best,qi_best_cont,train_true,cleaner=[0,1,2]);\n",
    "#0.00954,0.0106,0.0968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- train sax model :\n",
      "volume 2.298 is too small, set to 4\n",
      "volume 582.445 is too big, set to 580\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.37153141  1.1291468   0.06192786  3.99498324  0.04618147  3.99698685]\n",
      "fitting score 0.00932574186619\n",
      "score is 0.00932574186619\n",
      " ---- train sax countour model :\n",
      "volume 2.298 is too small, set to 4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 0.67875272  0.78012469  0.05782141  3.99983309  0.04654852  3.99673419]\n",
      "fitting score 0.00951856704943\n",
      "score is 0.00951856704943\n",
      " ---- train sax countour filter model :\n",
      "volume 2.298 is too small, set to 4\n",
      "volume 582.445 is too big, set to 580\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.531951    1.16518033  0.06392761  3.99947066  0.0474817   3.99174552]\n",
      "fitting score 0.00930264366878\n",
      "score is 0.00930264366878\n"
     ]
    }
   ],
   "source": [
    "pick = [4,5];\n",
    "qi_best,qi_best_cont = analysis.take_best_contour([qi_areas[i] for i in pick],[qi_cnts[i] for i in pick],method=1);\n",
    "qi_sax_pred3 = train_pred.train_sax_model(qi_best,train_true, version = 2,cleaner=[0,1,2]);\n",
    "qi_sax_cnt_pred3 = train_pred.train_sax_cnt_model(qi_best, qi_best_cont, train_true, version = 2,cleaner=[0,1,2]);\n",
    "qi_sax_filter_pred3 = train_pred.train_sax_cnt_filter_model(qi_best,qi_best_cont,train_true,cleaner=[0,1,2]);\n",
    "#0.00944,0.00958,0.00943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---- train sax model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.16989609  1.03810506  0.06294464  3.99626325  0.04821035  3.99756062]\n",
      "fitting score 0.00954991771869\n",
      "score is 0.00954991771869\n",
      " ---- train sax countour model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 0.58945593  0.72845071  0.06463916  4.00005477  0.05108148  3.99609428]\n",
      "fitting score 0.0101470311452\n",
      "score is 0.0101470311452\n",
      " ---- train sax countour filter model :\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "length <=4\n",
      "300 cases are used to fit the model\n",
      "fitting parameters [ 1.0685861   1.09247361  0.07148267  4.0024348   0.04916844  3.99205676]\n",
      "fitting score 0.0095641095916\n",
      "score is 0.0095641095916\n"
     ]
    }
   ],
   "source": [
    "pick = [6,7];\n",
    "qi_best,qi_best_cont = analysis.take_best_contour([qi_areas[i] for i in pick],[qi_cnts[i] for i in pick],method=1);\n",
    "qi_sax_pred4 = train_pred.train_sax_model(qi_best,train_true, version = 2,cleaner=[0,1,2]);\n",
    "qi_sax_cnt_pred4 = train_pred.train_sax_cnt_model(qi_best, qi_best_cont, train_true, version = 2,cleaner=[0,1,2]);\n",
    "qi_sax_filter_pred4 = train_pred.train_sax_cnt_filter_model(qi_best,qi_best_cont,train_true,cleaner=[0,1,2]);\n",
    "#0.00959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --------- average models --\n",
      "combine # predictions:700,700,700,700,700,700,700,700,700,700,700,700\n",
      "init score :0.00892194088431\n",
      "fitting parameters [ 3.28909099  2.62825157  3.80375984  3.92005645  3.25201615  2.36624647\n",
      "  3.24030371  3.95696622  3.12327609  4.06444979  4.22283368  3.66943437]\n",
      "fitting score 0.00888071067091\n",
      "combine # predictions:700,700,700,700,700,700,700,700,700,700,700,700\n",
      "score is 0.00884976414184\n"
     ]
    }
   ],
   "source": [
    "# fit the combined model based on the cnn-sax models.\n",
    "# when it fails, fall to the previously fitted fall back model\n",
    "print(\" --------- average models --\");\n",
    "reload(fitting_models)\n",
    "#qi_sax_cnt_pred4,qi_sax_filter_pred4,\n",
    "all_models = [qi_sax_pred,qi_sax_cnt_pred,\\\n",
    "             qi_sax_pred2,qi_sax_cnt_pred2,\\\n",
    "             qi_sax_pred3,qi_sax_cnt_pred3,\\\n",
    "             qi_sax_filter_pred,qi_sax_filter_pred2,qi_sax_filter_pred3,\\\n",
    "             tencia_predict,default_pred,qi_sax_pred4]; #validate = 0.0984\n",
    "\n",
    "# ave_model = fitting_models.AverageModel(2e-4);\n",
    "# all_models = [qi_sax_filter_pred, qi_sax_filter_pred2,qi_sax_filter_pred3,qi_sax_filter_pred4];\n",
    "# ave_model.fit(all_models, train_true)\n",
    "# qi_sax_filter_pred_ave = ave_model.predict(all_models);\n",
    "# analysis.evaluate_pred(qi_sax_filter_pred_ave,train_true)\n",
    "# all_models = [qi_sax_cnt_pred, qi_sax_cnt_pred2,qi_sax_cnt_pred3,qi_sax_cnt_pred4];\n",
    "# ave_model.fit(all_models, train_true)\n",
    "# qi_sax_cnt_pred_ave = ave_model.predict(all_models);\n",
    "# analysis.evaluate_pred(qi_sax_cnt_pred_ave,train_true)\n",
    "# all_models = [qi_sax_pred, qi_sax_pred2,qi_sax_pred3,qi_sax_pred4];\n",
    "# ave_model.fit(all_models, train_true)\n",
    "# qi_sax_pred_ave = ave_model.predict(all_models);\n",
    "# analysis.evaluate_pred(qi_sax_pred_ave,train_true)\n",
    "# all_models = [qi_sax_pred_ave,tencia_predict,default_pred,qi_sax_filter_pred_ave,qi_sax_cnt_pred_ave];\n",
    "reload(fitting_models)\n",
    "ave_model = fitting_models.AverageModel(1.0e-4);\n",
    "ave_model.fit(all_models,train_true);\n",
    "ave_model.set(ave_model.p*1.1)\n",
    "#N = len(all_models);\n",
    "#ave_model.set(np.array([np.sqrt(N)]*N)*0.95)\n",
    "ave_pred = ave_model.predict(all_models);\n",
    "\n",
    "#apply default model when it fails\n",
    "final_pred = analysis.fill_default(ave_pred, default_pred);\n",
    "analysis.evaluate_pred(final_pred, train_true);\n",
    "#analysis.evaluate_pred(final_pred, validate);\n",
    "\n",
    "#this is for the test part, the test cases are also calculated in the above trainning process\n",
    "#we might want to save the parameters of the models, and code another script to calculate the test cases. These fittings are very quick, so I'll leave it here and work on some other things first. \n",
    "#analysis.make_submit(final_pred, 501, 700, \"v2\"); #inclusive 500-700\n",
    "\n",
    "#1-300 train: 0.008920\n",
    "#201-500:validate: 0.009846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is 0.00987173461514\n"
     ]
    }
   ],
   "source": [
    "analysis.evaluate_pred(final_pred, validate); #0.00978"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score is 0.00893311843184\n"
     ]
    }
   ],
   "source": [
    "reload(analysis)\n",
    "analysis.evaluate_pred(final_pred, train_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.82121781,  2.45188259,  8.87643828,  3.66078859])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred.get(212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  94.29886892,   12.54399006,  255.17018585,   26.67766744])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_pred.get(595)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  37.95278885,    8.00086614,  114.42115483,   16.06057827])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_pred.get(599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  93.84516574   14.25015755  252.66617114   25.75961421]\n",
      "[ 103.12696935   14.25015755  262.36889918   26.67766744] [  79.92246034   14.35163556  238.11207909   25.75961421] [  75.   35.  181.   45.]\n",
      "Empty DataFrame\n",
      "Columns: [Id, Systole, Diastole]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Id, Systole, Diastole]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "case = 595;\n",
    "print final_pred.get(case)\n",
    "print oneslice_pred.get(case),ch4_predict.get(case),sa_predict.get(case)\n",
    "print train_true[train_true.Id==case]\n",
    "print validate[validate.Id==case]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.35303078   3.17128321  23.85628402   4.53848263]\n"
     ]
    }
   ],
   "source": [
    "print qi_sax_pred.get(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  59.94360765,   12.3034449 ,  170.25612443,   15.00278353])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch4_predict.get(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([1,2],80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
